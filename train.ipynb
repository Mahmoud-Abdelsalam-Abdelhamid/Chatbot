{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import nlp_utils\n",
    "from NN_model import NeuralNetwork"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load JSON file\n",
    "def load_intents(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        return json.load(file)\n",
    "\n",
    "# Save JSON file\n",
    "def save_intents(data, file_path):\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(data, file, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # File paths\n",
    "# input_file = \"intents_augmented_1.json\"\n",
    "# output_file = \"intents_augmented.json\"\n",
    "\n",
    "# # Load, Augment, and Save\n",
    "# intents = load_intents(input_file)\n",
    "# augmented_intents = nlp_utils.augment_intents(intents)\n",
    "# save_intents(augmented_intents, output_file)\n",
    "\n",
    "# print(f\"âœ… Data augmentation complete! Saved as {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "patterns = []  # Collect all patterns\n",
    "pattern_tags = []  # Store corresponding tags\n",
    "\n",
    "augmented_intents = load_intents(\"intents_augmented.json\")\n",
    "\n",
    "# Process each pattern\n",
    "for intent in augmented_intents[\"intents\"]:\n",
    "    for pattern in intent[\"patterns\"]:\n",
    "        patterns.append(pattern)  \n",
    "        pattern_tags.append(intent[\"tag\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text = [\"hello, How Are You, Friend?\", \"Tell me a funny joke:)\"]\n",
    "\n",
    "# processed_pattern = nlp_utils.remove_punctuation(text)\n",
    "# print(\"removed_punc:\", processed_pattern)\n",
    "\n",
    "# processed_pattern = nlp_utils.tokenize(processed_pattern)\n",
    "# print(\"tokenized:\", processed_pattern)\n",
    "\n",
    "# processed_pattern = nlp_utils.remove_stopwords(processed_pattern)\n",
    "# print(\"removed_stp:\", processed_pattern)\n",
    "\n",
    "# processed_pattern = nlp_utils.stem(processed_pattern)\n",
    "# print(\"stemmed:\", processed_pattern)\n",
    "\n",
    "# processed_pattern = nlp_utils.join_tokens(processed_pattern)\n",
    "# print(\"joined:\", processed_pattern)\n",
    "\n",
    "# processed_pattern = nlp_utils.get_embedding(processed_pattern)\n",
    "# print(\"embedded:\", len(processed_pattern))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = nlp_utils.create_pipeline()  # Load pipeline once\n",
    "processed_patterns = pipeline.transform(patterns)  # Transform all at once\n",
    "\n",
    "# word2vec_model = nlp_utils.train_word2vec(processed_patterns)\n",
    "\n",
    "# embedded_patterns = nlp_utils.word2vec_embeddings(processed_patterns, word2vec_model)\n",
    "\n",
    "# Apply GloVe embeddings to patterns\n",
    "# embedded_patterns = np.array([get_embedding(\" \".join(pattern)) for pattern in processed_patterns])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_to_index = {tag: idx for idx, tag in enumerate(sorted(set(pattern_tags)))}\n",
    "index_to_tag = {idx: tag for tag, idx in tag_to_index.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert training data into vectors\n",
    "X_data = np.array(processed_patterns)\n",
    "y_data = np.array([tag_to_index[tag] for tag in pattern_tags])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_data = scaler.fit_transform(X_data)  # Normalize input data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_data, y_data, test_size=0.2, random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1060, 200)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper-parameters \n",
    "num_epochs = 1000\n",
    "input_size = 200\n",
    "hidden_size = 128\n",
    "output_size = len(set(y_train))\n",
    "\n",
    "class ChatDataset(Dataset):\n",
    "    def __init__(self, X_data, y_data):\n",
    "        self.n_samples = len(X_data)\n",
    "        self.xs = torch.tensor(X_data, dtype=torch.float32).to(device)\n",
    "        self.ys = torch.tensor(y_data, dtype=torch.long).to(device)\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        return self.xs[idx], self.ys[idx]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.n_samples\n",
    "\n",
    "\n",
    "# Create dataset objects\n",
    "train_dataset = ChatDataset(X_train, y_train)\n",
    "val_dataset = ChatDataset(X_val, y_val)\n",
    "\n",
    "# Create DataLoader objects\n",
    "train_loader = DataLoader(dataset=train_dataset, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(dataset=val_dataset, shuffle=False, num_workers=0)  # No shuffle for validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\LENOVO\\anaconda3\\envs\\AI_env\\lib\\site-packages\\torch\\optim\\lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNetwork(input_size, hidden_size, output_size).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=0.0001, weight_decay=1e-4)  # More stable optimizer\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=35, factor=0.5, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/1000], Loss: 2.0289, Accuracy: 0.5019, Val Loss: 1.4334, Val Accuracy: 0.7293\n",
      "Epoch [10/1000], Loss: 1.1244, Accuracy: 0.7349, Val Loss: 0.6690, Val Accuracy: 0.8647\n",
      "Epoch [15/1000], Loss: 0.6742, Accuracy: 0.8387, Val Loss: 0.4195, Val Accuracy: 0.8985\n",
      "Epoch [20/1000], Loss: 0.4993, Accuracy: 0.8679, Val Loss: 0.3433, Val Accuracy: 0.9098\n",
      "Epoch [25/1000], Loss: 0.4374, Accuracy: 0.8792, Val Loss: 0.3212, Val Accuracy: 0.9098\n",
      "Epoch [30/1000], Loss: 0.3559, Accuracy: 0.8972, Val Loss: 0.2835, Val Accuracy: 0.9211\n",
      "Epoch [35/1000], Loss: 0.3377, Accuracy: 0.9123, Val Loss: 0.2696, Val Accuracy: 0.9248\n",
      "Epoch [40/1000], Loss: 0.2964, Accuracy: 0.9160, Val Loss: 0.2533, Val Accuracy: 0.9173\n",
      "Epoch [45/1000], Loss: 0.2578, Accuracy: 0.9302, Val Loss: 0.2646, Val Accuracy: 0.9211\n",
      "Epoch [50/1000], Loss: 0.2607, Accuracy: 0.9208, Val Loss: 0.2204, Val Accuracy: 0.9474\n",
      "Epoch [55/1000], Loss: 0.2790, Accuracy: 0.9198, Val Loss: 0.2343, Val Accuracy: 0.9361\n",
      "Epoch [60/1000], Loss: 0.2346, Accuracy: 0.9264, Val Loss: 0.2369, Val Accuracy: 0.9361\n",
      "Epoch [65/1000], Loss: 0.2374, Accuracy: 0.9302, Val Loss: 0.2350, Val Accuracy: 0.9361\n",
      "Epoch [70/1000], Loss: 0.2078, Accuracy: 0.9377, Val Loss: 0.2363, Val Accuracy: 0.9286\n",
      "Epoch [75/1000], Loss: 0.2190, Accuracy: 0.9330, Val Loss: 0.2409, Val Accuracy: 0.9323\n",
      "Epoch [80/1000], Loss: 0.1960, Accuracy: 0.9425, Val Loss: 0.2497, Val Accuracy: 0.9323\n",
      "Epoch [85/1000], Loss: 0.2053, Accuracy: 0.9264, Val Loss: 0.2430, Val Accuracy: 0.9436\n",
      "Epoch [90/1000], Loss: 0.1850, Accuracy: 0.9396, Val Loss: 0.2381, Val Accuracy: 0.9361\n",
      "Epoch [95/1000], Loss: 0.2040, Accuracy: 0.9443, Val Loss: 0.2272, Val Accuracy: 0.9323\n",
      "Early stopping triggered after 97 epochs!\n",
      "Training complete! Best validation loss: 0.2137803057068896\n"
     ]
    }
   ],
   "source": [
    "# Early stopping setup\n",
    "patience = 25\n",
    "best_val_loss = np.inf\n",
    "epochs_no_improve = 0\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    epoch_loss, epoch_accuracy, total_samples = 0.0, 0.0, 0\n",
    "\n",
    "    for words, labels in train_loader:\n",
    "        words, labels = words.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(words)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        batch_size = labels.size(0)\n",
    "        batch_accuracy = (outputs.argmax(dim=1) == labels).sum().item() / batch_size\n",
    "        epoch_loss += loss.item() * batch_size\n",
    "        epoch_accuracy += batch_accuracy * batch_size\n",
    "        total_samples += batch_size\n",
    "\n",
    "    epoch_loss /= total_samples\n",
    "    epoch_accuracy /= total_samples\n",
    "\n",
    "    # Validation Phase\n",
    "    model.eval()\n",
    "    val_loss, val_accuracy, val_samples = 0.0, 0.0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for words, labels in val_loader:\n",
    "            words, labels = words.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(words)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            batch_size = labels.size(0)\n",
    "            batch_accuracy = (outputs.argmax(dim=1) == labels).sum().item() / batch_size\n",
    "            val_loss += loss.item() * batch_size\n",
    "            val_accuracy += batch_accuracy * batch_size\n",
    "            val_samples += batch_size\n",
    "\n",
    "    val_loss /= val_samples\n",
    "    val_accuracy /= val_samples\n",
    "\n",
    "    # Check for early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        epochs_no_improve = 0\n",
    "        torch.save(model.state_dict(), \"best_model.pth\")  # Save best model\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "\n",
    "    if (epoch + 1) % 5 == 0:\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], '\n",
    "              f'Loss: {epoch_loss:.4f}, Accuracy: {epoch_accuracy:.4f}, '\n",
    "              f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')\n",
    "\n",
    "    if epochs_no_improve >= patience:\n",
    "        print(f\"Early stopping triggered after {epoch+1} epochs!\")\n",
    "        break\n",
    "\n",
    "    scheduler.step(val_loss)  # Adjust LR\n",
    "\n",
    "print(\"Training complete! Best validation loss:\", best_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training complete. file saved to data_0.94_0.94.pth\n"
     ]
    }
   ],
   "source": [
    "data = {\n",
    "\"model_state\": model.state_dict(),\n",
    "\"input_size\": input_size,\n",
    "\"hidden_size\": hidden_size,\n",
    "\"output_size\": output_size,\n",
    "\"all_words\": patterns,\n",
    "\"tags\": sorted(set(pattern_tags))\n",
    "}\n",
    "\n",
    "FILE = f\"data_{round(epoch_accuracy, 2)}_{round(val_accuracy, 2)}.pth\"\n",
    "torch.save(data, FILE)\n",
    "\n",
    "print(f'training complete. file saved to {FILE}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline has been saved successfully!\n",
      "Scaler saved successfully!\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(pipeline, \"pipeline.pkl\")\n",
    "print(\"Pipeline has been saved successfully!\")\n",
    "\n",
    "\n",
    "joblib.dump(scaler, \"scaler.pkl\")\n",
    "print(\"Scaler saved successfully!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AI_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
